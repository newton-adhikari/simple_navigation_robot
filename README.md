# ðŸ¤– TurtleBot3 Reinforcement Learning Navigation

This project implements a reinforcement learning (RL) navigation agent for **TurtleBot3 Burger** in **Gazebo** using **ROS2** and **Gymnasium**. The agent is trained with **Stable-Baselines3 PPO** to navigate toward a goal while avoiding obstacles.

---

## ðŸš€ How It Works

### Environment (`simulate_turtlebot.py`)

- **Actions:**  
  Continuous actions normalized to `[-1,1]`, mapped to robot velocities:

  - Linear velocity:  
    `v = (a_lin + 1)/2 * v_max,   v_max = 0.22 m/s`

  - Angular velocity:  
    `Ï‰ = a_ang * Ï‰_max,   Ï‰_max = 2.84 rad/s`

- **Observations:**  
  360â€‘dimensional LIDAR scan, normalized:  
  `obs_i = clip(r_i, 0, 10) / 10`

- **Reset:**  
  - Stops the robot  
  - Calls `/reset_simulation`  
  - Waits for fresh `/scan` and `/odom`  
  - Initializes distance to goal:  
    `d0 = || g - p0 ||_2`

- **Step:**  
  - Publishes velocity command  
  - Spins ROS2 once  
  - Computes reward and termination  

---

## ðŸ§® Reward Function (Mathematical Summary)

The total reward is shaped as:

`R = R_progress + R_heading + R_goal + R_collision + R_prox + R_time + R_survival`

- **Progress toward goal:**  
  `R_progress = Î± * (d_{t-1} - d_t),   Î± â‰ˆ 5â€“20`

- **Heading alignment:**  
  `R_heading = Î² * (Ï€ - |Î”Î¸|),   Î”Î¸ = wrap(Ï† - Î¸)`

- **Goal bonus:**  
  `R_goal = B   if d_t < Îµ_goal,   B âˆˆ [50, 100]`

- **Collision penalty:**  
  `R_collision = -C   if d_min < Îµ_collision,   C âˆˆ [10, 20]`

- **Smooth proximity penalty:**  
  `R_prox = -Î³ / (d_min + Îµ)`

- **Time shaping:**  
  `R_time = -0.01,   R_survival = +0.05`

---

## ðŸ”£ Symbols

Hereâ€™s what each symbol means in the equations above:

- `v` â†’ Linear velocity of the robot (m/s)  
- `Ï‰` â†’ Angular velocity of the robot (rad/s)  
- `a_lin` â†’ Normalized linear action chosen by the agent (`[-1,1]`)  
- `a_ang` â†’ Normalized angular action chosen by the agent (`[-1,1]`)  
- `v_max` â†’ Maximum forward speed of TurtleBot3 Burger (`0.22 m/s`)  
- `Ï‰_max` â†’ Maximum angular speed of TurtleBot3 Burger (`2.84 rad/s`)  
- `obs_i` â†’ Normalized LIDAR reading for beam `i`  
- `r_i` â†’ Raw LIDAR distance reading for beam `i` (meters)  
- `d_t` â†’ Distance from robot to goal at timestep `t`  
- `d_{t-1}` â†’ Distance to goal at previous timestep  
- `d0` â†’ Initial distance to goal at episode start  
- `g` â†’ Goal position vector `[g_x, g_y]`  
- `p0` â†’ Initial robot position vector `[x_0, y_0]`  
- `Î”Î¸` â†’ Difference between robotâ€™s heading and goal direction (radians)  
- `Ï†` â†’ Bearing angle from robot to goal (radians)  
- `Î¸` â†’ Robotâ€™s current yaw angle (radians)  
- `d_min` â†’ Minimum LIDAR distance (closest obstacle in meters)  
- `Îµ_goal` â†’ Goal threshold distance (â‰ˆ 0.30 m)  
- `Îµ_collision` â†’ Collision threshold distance (â‰ˆ 0.20 m)  
- `Î±` â†’ Scaling factor for progress reward (â‰ˆ 5â€“20)  
- `Î²` â†’ Scaling factor for heading alignment reward  
- `B` â†’ Goal completion bonus (â‰ˆ 50â€“100)  
- `C` â†’ Collision penalty (â‰ˆ 10â€“20)  
- `Î³` â†’ Scaling factor for smooth proximity penalty  
- `Îµ` â†’ Small constant to avoid division by zero  
- `R_time` â†’ Small negative reward per step (time penalty)  
- `R_survival` â†’ Small positive reward per step (survival bonus)  

---

## ðŸ‹ï¸ Training (`test_agent.py`)

- Uses **PPO** with MLP policy.
- Minimal example to train:

  ```bash
  python3 test_agent.py
    ```
Saves model to ppo_turtlebot3.zip.

---

## âš ï¸ For meaningful results, increase timesteps:
model.learn(total_timesteps=200000)

---

## â–¶ï¸ Running Trained Agent (run_trained_agent.py)
```bash
export TURTLEBOT3_MODEL=burger
ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py

python3 run_trained_agent.py

```

## ðŸ“Š Training Recommendations
Timesteps: 200kâ€“1M for stable navigation.

VecNormalize: Normalize observations and rewards for stability.

VecEnvs: Use DummyVecEnv or SubprocVecEnv for parallel rollouts.

Curriculum: Start with simple worlds, then increase complexity.

Randomized goals: Prevent overfitting to a single target.

---

## ðŸ“‚ Project Structure
```plaintext
/src
â”œâ”€â”€ run_trained_agent.py
â”œâ”€â”€ simulate_turtlebot.py
â””â”€â”€ test_agent.py
```
